🗓️ Log: 2025-06-24
✅ 今日进展

    Diffusion Policy 成功输出目标位置：

        所有 Agent 的 DP Output 均准确返回目标点坐标；

        已验证 DP 正确接入模型，并在控制循环中持续运行；

        各 Agent 的输出 pred_xy 与 target_xy 完全一致，误差为 0.000，说明当前 DP 模型是硬编码目标坐标。

    确认模型未学习实际策略：

        DP 模型输出固定目标位置，而非根据当前 observation 推理；

        原因在于当前训练所用 obs 缺乏目标位置信息，因此 DP 无法学会任何 goal-directed policy；

        结论：目前接入的模型虽然结构正确，但行为无效（策略未生效）。

    明确 DP obs 的维度限制问题：

        当前模型为 20 维输入，若新增 target_pos 信息将导致维度不匹配；

        若要引入目标信息，必须重新组织 obs 保持 20 维，或重新训练模型；

    尝试加入目标信息失败：

        测试中尝试扩展 obs 结构失败，运行时报错维度不匹配；

        最终确认不能直接“加”维度，只能“替换”部分字段。

🔍 遇到的问题

    DP模型行为看似合理，实则并未学习；

    obs 维度固定，添加目标信息需谨慎处理；

    DP Wrapper 封装逻辑已通畅，但预测值只是映射目标，并未体现“预测”。

📌 明日计划

设计新的 obs layout，保留目标信息同时控制在 20 维；

用 NearestTaskPolicy 生成新的 goal-conditioned 数据；

重训练 Diffusion Policy 模型；

为主程序加入 agent 是否“到达目标”的正式检测逻辑；

整理代码结构，准备后续论文材料。